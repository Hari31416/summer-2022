{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Contents\">Contents<a href=\"#Contents\"></a></h2>\n",
    "        <ol>\n",
    "        <li><a class=\"\" href=\"#Linear-Model-Selection-and-Regularization\">Linear Model Selection and Regularization</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Subset-Selection\">Subset Selection</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Best-Subset-Selection\">Best Subset Selection</a></li>\n",
    "<li><a class=\"\" href=\"#Stepwise-Selection\">Stepwise Selection</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Forward-Stepwise-Selection\">Forward Stepwise Selection</a></li>\n",
    "<li><a class=\"\" href=\"#Backward-Stepwise-Selection\">Backward Stepwise Selection</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Choosing-the-Optimal-Model\">Choosing the Optimal Model</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Shrinkage-Methods\">Shrinkage Methods</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Ridge-Regression\">Ridge Regression</a></li>\n",
    "<li><a class=\"\" href=\"#The-Lasso\">The Lasso</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Dimension-Reduction-Methods\">Dimension Reduction Methods</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Principal-Components-Regression\">Principal Components Regression</a></li>\n",
    "<li><a class=\"\" href=\"#Partial-Least-Squares\">Partial Least Squares</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X_1+ \\beta_2 X_2+\\cdots + \\epsilon\n",
    "$$\n",
    "is fitted using the least squares method. In a lot of cases, this method gives good result, however, it is not always the case. Using some other method to fit the model may result in better *prediction accuracy* and *model interpretability*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Prediction accuracy:** Provided that the true relationship between the\n",
    "response and the predictors is approximately linear, the least squares\n",
    "estimates will have low bias if $n>>p$ hence it will generalize well on test data. If n is not much larger than p, the bias will be greater and if $n<p$ there\n",
    "is no longer a unique least squares coefficient estimate: the variance\n",
    "is infinite so the method cannot be used at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Model interpretability:**  It is often the case that some or many of the\n",
    "variables used in a multiple regression model are in fact not associated with the response. Including such irrelevant variables leads to\n",
    "unnecessary complexity in the resulting model. By removing these\n",
    "variables—that is, by setting the corresponding coefficient estimates\n",
    "to zero—we can obtain a model that is more easily interpreted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many alternatives, both classical and modern, to using least\n",
    "squares to fit. Some are:\n",
    "*  **Subset Selection:** This approach involves identifying a subset of the p\n",
    "predictors that we believe to be related to the response. We then fit\n",
    "a model using least squares on the reduced set of variables.\n",
    "* **Shrinkage:** This approach involves fitting a model involving all p predictors. However, the estimated coefficients are shrunken towards zero\n",
    "relative to the least squares estimates.\n",
    "* **Dimension Reduction:** This approach involves projecting the p predictors into a M-dimensional subspace, where $M < p$. This is achieved\n",
    "by computing M different linear combinations, or projections, of the\n",
    "variables. Then these M projections are used as predictors to fit a\n",
    "linear regression model by least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform best subset selection, we fit a separate least squares regression\n",
    "for each possible combination of the p predictors. That is, we fit all p models selection\n",
    "that contain exactly one predictor, all $^pP_2 = p(p-1)/2$ models that contain\n",
    "exactly two predictors, and so forth. We then look at all of the resulting\n",
    "models, with the goal of identifying the one that is best.\n",
    "The problem of selecting the best model from among the $2^p$ possibilities\n",
    "considered by best subset selection is not trivial. This is usually broken up\n",
    "into two stages, as described in Algorithm:\n",
    "1. Let $M_0$ denote the null model, which contains no predictors. This\n",
    "model simply predicts the sample mean for each observation.\n",
    "2. For $k = 1, 2, \\cdots p$:\n",
    "    - Fit all $^pP_k$ models that contain exactly k predictors.\n",
    "\n",
    "    - Pick the best among these $^pP_k$ models, and call it $M_k$.\n",
    "\n",
    "3. Select a single best model from among $M_0, M_1, \\cdots, M_p$ using cross-validated prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are $2^p$ possible models, this type of selection becomes computationally expensive for large $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computational reasons, best subset selection cannot be applied with\n",
    "very large p. Best subset selection may also suffer from statistical problems\n",
    "when p is large. The larger the search space, the higher the chance of finding\n",
    "models that look good on the training data, even though they might not\n",
    "have any predictive power on future data. Thus an enormous search space\n",
    "can lead to overfitting and high variance of the coefficient estimates.\n",
    "For both of these reasons, stepwise methods, which explore a far more\n",
    "restricted set of models, are attractive alternatives to best subset selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward stepwise selection\n",
    "begins with a model containing no predictors, and then adds predictors\n",
    "to the model, one-at-a-time, until all of the predictors are in the model.\n",
    "In particular, at each step the variable that gives the greatest additional\n",
    "improvement to the fit is added to the model. More formally, the forward\n",
    "stepwise selection procedure is given in Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let $M_0$ denote the null model, which contains no predictors.\n",
    "2. For $k = 0, \\cdots , p − 1$:\n",
    "    - Consider all $p − k$ models that augment the predictors in $M_k$ with one additional predictor.\n",
    "    - Choose the best among these $p − k$ models, and call it $M_{k+1}$. Here best is defined as having smallest RSS or highest $R^2$.\n",
    "3. Select a single best model from among $M_0, \\cdots , M_p$ using crossvalidated prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm amounts in total $1+p(p+1)/2$ models, in place of $2^p$ models that are required by best subset selection. This is a substantial difference: when\n",
    "p = 20, best subset selection requires fitting 1,048,576 models, whereas\n",
    "forward stepwise selection requires fitting only 211 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though forward stepwise tends to do well in practice,\n",
    "it is not guaranteed to find the best possible model out of all $2^p$ models containing subsets of the p predictors. For instance, suppose that in a\n",
    "given data set with $p = 3$ predictors, the best possible one-variable model\n",
    "contains $X_1$, and the best possible two-variable model instead contains $X_2$\n",
    "and $X_3$. Then forward stepwise selection will fail to select the best possible\n",
    "two-variable model, because $M_1$ will contain $X_1$, so $M_2$ must also contain\n",
    "$X_1$ together with one additional variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Algorithm is:\n",
    "1. Let M\n",
    "p denote the full model, which contains all p predictors.\n",
    "2. For $k = p, p − 1, \\cdots , 1$:\n",
    "    - Consider all k models that contain all but one of the predictors in $M_k$, for a total of k − 1 predictors.\n",
    "    - Choose the best among these k models, and call it $M_k−1$. Here best is defined as having smallest RSS or highest $R_2$.\n",
    "3. Select a single best model from among $M_0, M_1, \\cdots , M_p$ using crossvalidated prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like forward stepwise selection, the backward selection approach searches\n",
    "through only $1+p(p+1)/2$ models, and so can be applied in settings where\n",
    "p is too large to apply best subset selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Optimal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train error can not be a good predictor of the model's performance. Instead, we should use the model's performance on the test data. There are two common approaches:\n",
    "1. We can indirectly estimate test error by making an adjustment to the\n",
    "training error to account for the bias due to overfitting.\n",
    "2. We can directly estimate the test error, using either a validation set\n",
    "approach or a cross-validation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinkage Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an\n",
    "alternative to the subset selection, we can fit a model containing all p predictors using a technique\n",
    "that constrains or regularizes the coefficient estimates, or equivalently, that\n",
    "shrinks the coefficient estimates towards zero.\n",
    "\n",
    "The two best-known techniques for shrinking the regression coefficients\n",
    "towards zero are ridge regression and the lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression is very similar to least squares, except that the coefficients\n",
    "ridge\n",
    "are estimated by minimizing a slightly different quantity. Here, we minimize the quantity:\n",
    "$$\n",
    "RSS + \\lambda \\sum_{i=1}^p \\beta_i^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $RSS$ is the sum of squares of the residuals, and $\\lambda \\ge 0$ is a tuning parameter to be determined separately.\n",
    "\n",
    "As with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS\n",
    "small. However, the second term, called a shrinkage penalty, is shrinkage\n",
    "small when $\\beta _1, \\beta _2, \\cdots \\beta _p$ are close to zero, and so it has the effect of shrinking penalty\n",
    "the estimates of $\\beta_j$ towards zero. The tuning parameter $\\lambda$ serves to control\n",
    "the relative impact of these two terms on the regression coefficient estimates. When $\\lambda =0$, the penalty term has no effect, and ridge regression\n",
    "will produce the least squares estimates. However, as $\\lambda \\to \\infty $, the impact of\n",
    "the shrinkage penalty grows, and the ridge regression coefficient estimates\n",
    "will approach zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the penlaty is not applied to the intercept term as we want to shrink the estimated association of\n",
    "each variable with the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression does have one obvious disadvantage. Unlike best subset,\n",
    "forward stepwise, and backward stepwise selection, which will generally\n",
    "select models that involve just a subset of the variables, ridge regression\n",
    "will include all p predictors in the final model. The penalty defined above\n",
    "will shrink all of the coefficients towards zero, but it will not set any of them\n",
    "exactly to zero. A solution to this problem is to use the lasso. In this, the penalty is defined as:\n",
    "$$\n",
    "RSS + \\lambda \\sum_{i=1}^p \\|\\beta_i\\|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the lasso uses $l_1$ penalty while ridge regression uses $l_2$ penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like best subset selection, the lasso performs variable selection. As a result, models generated\n",
    "from the lasso are generally much easier to interpret than those produced\n",
    "by ridge regression. We say that the lasso yields sparse models—that is,\n",
    "models that involve only a subset of the variables. As in ridge regression, selecting good value of $\\lambda$ is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now explore\n",
    "a class of approaches that transform the predictors and then fit a least\n",
    "squares model using the transformed variables. We will refer to these techniques as dimension reduction methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Z_1, Z_2, \\cdots , Z_M$ represent $M \\le p$ linear combinations of our original reduction\n",
    "linear\n",
    "combination\n",
    "p predictors. That is:\n",
    "$$\n",
    "Z_m = \\sum_{j=1}^p \\phi_{jm} Z_j\n",
    "$$\n",
    "We then fit the linear regression model\n",
    "$$\n",
    "y_i = \\theta _0+ \\sum_{m=1}^M\\theta _m z_{im} \\epsilon_i\n",
    "$$\n",
    "Using the least squares method. If the constants $\\phi_{1m}, \\phi_{2m}, \\cdots , \\phi_{pm}$ are chosen wisely, then\n",
    "such dimension reduction approaches can often outperform least squares\n",
    "regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the number of coeeficients to estimate is decreased from p+1 to M+1 is what gives it the name of dimension reduction. Note that the original coeeficients $\\beta _j$ can be calculated as:\n",
    "$$\n",
    "\\beta _j = \\sum_{m=1}^M\\theta _m \\phi_{jm}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, dimension reduction techniques can be thought of as a way to constraint the values $\\beta_j$ can take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal components analysis (PCA) is a popular approach for deriving\n",
    "principal\n",
    "components\n",
    "analysis\n",
    "a low-dimensional set of features from a large set of variables. PCA is a technique for reducing the dimension of a n × p data matrix X.\n",
    "The first principal component direction of the data is that along which the\n",
    "observations vary the most. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/06_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure, the green line shows the maximum variance of the data. In general, one can construct up to p distinct principal components. The second\n",
    "principal component $Z_2$ is a linear combination of the variables that is uncorrelated with $Z_1$, and has largest variance subject to this constraint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal components regression (PCR) approach involves constructing\n",
    "principal\n",
    "components\n",
    "regression\n",
    "the first M principal components, $Z_1, Z_2, \\cdots , Z_M$, and then using these components as the predictors in a linear regression model that is fit\n",
    "using least squares. The key idea is that often a small number of principal components suffice to explain most of the variability in the data, as\n",
    "well as the relationship with the response. In other words, we assume that\n",
    "the directions in which $X_1,\\cdots , X_p$ show the most variation are the directions that are associated with Y. If the assumption underlying PCR holds, then fitting a least squares\n",
    "model to $Z_1, Z_2, \\cdots , Z_M$ will lead to better results than fitting a least squares\n",
    "model to $X_1,\\cdots , X_p$, since most or all of the information in the data that\n",
    "relates to the response is contained in $Z_1, Z_2, \\cdots , Z_M$, and by estimating only\n",
    "M << p coefficients we can mitigate overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCR approach that we just described involves identifying linear combinations, or directions, that best represent the predictors $X_1,\\cdots , X_p$. These\n",
    "directions are identified in an unsupervised way, since the response Y is not\n",
    "used to help determine the principal component directions. That is, the\n",
    "response does not supervise the identification of the principal components.\n",
    "Consequently, PCR suffers from a drawback: there is no guarantee that the\n",
    "directions that best explain the predictors will also be the best directions\n",
    "to use for predicting the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial least squares (PLS), is a supervised alternative to\n",
    "partial least\n",
    "PCR. Like PCR, PLS is a dimension reduction method, which first identifies squares\n",
    "a new set of features $Z_1, Z_2, \\cdots , Z_M$ that are linear combinations of the original\n",
    "features, and then fits a linear model via least squares using these M new\n",
    "features. But unlike PCR, PLS identifies these new features in a supervised\n",
    "way—that is, it makes use of the response Y in order to identify new\n",
    "features that not only approximate the old features well, but also that are\n",
    "related to the response. Roughly speaking, the PLS approach attempts to\n",
    "find directions that help explain both the response and the predictors."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
