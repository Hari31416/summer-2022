{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X_1+ \\beta_2 X_2+\\cdots + \\epsilon\n",
    "$$\n",
    "is fitted using the least squares method. In a lot of cases, this method gives good result, however, it is not always the case. Using some other method to fit the model may result in better *prediction accuracy* and *model interpretability*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Prediction accuracy:** Provided that the true relationship between the\n",
    "response and the predictors is approximately linear, the least squares\n",
    "estimates will have low bias if $n>>p$ hence it will generalize well on test data. If n is not much larger than p, the bias will be greater and if $n<p$ there\n",
    "is no longer a unique least squares coefficient estimate: the variance\n",
    "is infinite so the method cannot be used at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Model interpretability:**  It is often the case that some or many of the\n",
    "variables used in a multiple regression model are in fact not associated with the response. Including such irrelevant variables leads to\n",
    "unnecessary complexity in the resulting model. By removing these\n",
    "variables—that is, by setting the corresponding coefficient estimates\n",
    "to zero—we can obtain a model that is more easily interpreted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many alternatives, both classical and modern, to using least\n",
    "squares to fit. Some are:\n",
    "*  **Subset Selection:** This approach involves identifying a subset of the p\n",
    "predictors that we believe to be related to the response. We then fit\n",
    "a model using least squares on the reduced set of variables.\n",
    "* **Shrinkage:** This approach involves fitting a model involving all p predictors. However, the estimated coefficients are shrunken towards zero\n",
    "relative to the least squares estimates.\n",
    "* **Dimension Reduction:** This approach involves projecting the p predictors into a M-dimensional subspace, where $M < p$. This is achieved\n",
    "by computing M different linear combinations, or projections, of the\n",
    "variables. Then these M projections are used as predictors to fit a\n",
    "linear regression model by least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform best subset selection, we fit a separate least squares regression\n",
    "for each possible combination of the p predictors. That is, we fit all p models selection\n",
    "that contain exactly one predictor, all $^pP_2 = p(p-1)/2$ models that contain\n",
    "exactly two predictors, and so forth. We then look at all of the resulting\n",
    "models, with the goal of identifying the one that is best.\n",
    "The problem of selecting the best model from among the $2^p$ possibilities\n",
    "considered by best subset selection is not trivial. This is usually broken up\n",
    "into two stages, as described in Algorithm:\n",
    "1. Let $M_0$ denote the null model, which contains no predictors. This\n",
    "model simply predicts the sample mean for each observation.\n",
    "2. For $k = 1, 2, \\cdots p$:\n",
    "    - Fit all $^pP_k$ models that contain exactly k predictors.\n",
    "\n",
    "    - Pick the best among these $^pP_k$ models, and call it $M_k$.\n",
    "\n",
    "3. Select a single best model from among $M_0, M_1, \\cdots, M_p$ using cross-validated prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are $2^p$ possible models, this type of selection becomes computationally expensive for large $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computational reasons, best subset selection cannot be applied with\n",
    "very large p. Best subset selection may also suffer from statistical problems\n",
    "when p is large. The larger the search space, the higher the chance of finding\n",
    "models that look good on the training data, even though they might not\n",
    "have any predictive power on future data. Thus an enormous search space\n",
    "can lead to overfitting and high variance of the coefficient estimates.\n",
    "For both of these reasons, stepwise methods, which explore a far more\n",
    "restricted set of models, are attractive alternatives to best subset selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward stepwise selection\n",
    "begins with a model containing no predictors, and then adds predictors\n",
    "to the model, one-at-a-time, until all of the predictors are in the model.\n",
    "In particular, at each step the variable that gives the greatest additional\n",
    "improvement to the fit is added to the model. More formally, the forward\n",
    "stepwise selection procedure is given in Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let $M_0$ denote the null model, which contains no predictors.\n",
    "2. For $k = 0, \\cdots , p − 1$:\n",
    "    - Consider all $p − k$ models that augment the predictors in $M_k$ with one additional predictor.\n",
    "    - Choose the best among these $p − k$ models, and call it $M_{k+1}$. Here best is defined as having smallest RSS or highest $R^2$.\n",
    "3. Select a single best model from among $M_0, \\cdots , M_p$ using crossvalidated prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm amounts in total $1+p(p+1)/2$ models, in place of $2^p$ models that are required by best subset selection. This is a substantial difference: when\n",
    "p = 20, best subset selection requires fitting 1,048,576 models, whereas\n",
    "forward stepwise selection requires fitting only 211 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though forward stepwise tends to do well in practice,\n",
    "it is not guaranteed to find the best possible model out of all $2^p$ models containing subsets of the p predictors. For instance, suppose that in a\n",
    "given data set with $p = 3$ predictors, the best possible one-variable model\n",
    "contains $X_1$, and the best possible two-variable model instead contains $X_2$\n",
    "and $X_3$. Then forward stepwise selection will fail to select the best possible\n",
    "two-variable model, because $M_1$ will contain $X_1$, so $M_2$ must also contain\n",
    "$X_1$ together with one additional variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Algorithm is:\n",
    "1. Let M\n",
    "p denote the full model, which contains all p predictors.\n",
    "2. For $k = p, p − 1, \\cdots , 1$:\n",
    "    - Consider all k models that contain all but one of the predictors in $M_k$, for a total of k − 1 predictors.\n",
    "    - Choose the best among these k models, and call it $M_k−1$. Here best is defined as having smallest RSS or highest $R_2$.\n",
    "3. Select a single best model from among $M_0, M_1, \\cdots , M_p$ using crossvalidated prediction error."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
