{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning deals with a set of statistical tools intended for the setting in which we have only a set of features $X_1, X_2, \\ldots , X_p$ measured on n observations. We are not interested\n",
    "in prediction, because we do not have an associated response variable Y .\n",
    "Rather, the goal is to discover interesting things about the measurements\n",
    "on $X_1, X_2, \\ldots , X_p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are Principal Components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The\n",
    "idea is that each of the n observations lives in p-dimensional space, but not\n",
    "all of these dimensions are equally interesting. PCA seeks a small number\n",
    "of dimensions that are as interesting as possible, where the concept of interesting is measured by the amount that the observations vary along each\n",
    "dimension. Each of the dimensions found by PCA is a linear combination\n",
    "of the p features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first principal component of a set of features $X_1, X_2, \\ldots , X_p$ is the\n",
    "normalized linear combination of the features:\n",
    "$$\n",
    "Z_1 = \\phi_{11}X_1 + \\phi_{21}X_2 + \\ldots + \\phi_{p1}X_p\\\\\n",
    "\\sum_{i=1}^p \\phi_{i1}^2 = 1\n",
    "$$\n",
    "The elements $\\phi_{11}, \\ldots, \\phi_{p1}$ are called the *loading* of the first principal. These can be calculated by minimizing the variance subjected to the constraint that the sum of the square of loading is 1.\n",
    "\n",
    ">The loading forms a vector $\\phi$ and they point in the directions in feature space along which the data vary the most,\n",
    "and the principal component scores as projections along these directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first principal component $Z_1$ of the features has been determined, we can find the second principal component $Z_2$. The second principal component is the linear combination of $X_1, X_2, \\ldots , X_p$ that has maximal\n",
    "variance out of all linear combinations that are uncorrelated with $Z_1$. The\n",
    "second principal component scores $z_{12}, z_{22}, \\cdots , z_{n2}$ take the form:\n",
    "$$\n",
    "z_{12} = \\phi_{12}x_{i1} + \\phi_{22}x_{i2} + \\ldots + \\phi_{p2}x_{ip}\n",
    "$$\n",
    "It turns out that constraining $Z_2$ to be uncorrelated with\n",
    "$Z_1$ is equivalent to constraining the direction $\\phi_2$ to be orthogonal (perpendicular) to the direction $\\phi_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have computed the principal components, we can plot them\n",
    "against each other in order to produce low-dimensional views of the data.\n",
    "For instance, we can plot the score vector $Z_1$ against $Z_2$, $Z_1$ against $Z_3$,\n",
    "$Z_2$ against Z3, and so forth. Geometrically, this amounts to projecting\n",
    "the original data down onto the subspace spanned by $\\phi_{1}$, $\\phi_{2}$, and $\\phi_{3}$, and\n",
    "plotting the projected points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results obtained\n",
    "when we perform PCA will also depend on whether the variables have been\n",
    "individually scaled, that is, if the features are scaled, the PCA will also change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniqueness of the Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each principal component loading vector is unique, up to a sign flip. This\n",
    "means that two different software packages will yield the same principal\n",
    "component loading vectors, although the signs of those loading vectors\n",
    "may differ. The signs may differ because each principal component loading\n",
    "vector specifies a direction in p-dimensional space: flipping the sign has no\n",
    "effect as the direction does not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Proportion of Variance Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total variance present in a data set (assuming\n",
    "that the variables have been centered to have mean zero) is defined as:\n",
    "$$\n",
    "\\sum_{i=1}^p \\text{Var}(X_j) = \\sum_{j=1}^p\\frac{1}{n}\\sum_{i=1}^n x_{ij}^2\n",
    "$$\n",
    "While the variance explained by the first principal component is:\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i=1}^n z_{im}^2 = \\frac{1}{n}\\sum_{i=1}^n\\left( \\sum_{j=1}^p\\phi_{jm}x_{ij} \\right)^2\n",
    "$$\n",
    "and hence, PVE is:\n",
    "$$\n",
    "\\frac{\\sum_{i=1}^p \\left( \\sum_{j=1}^p\\phi_{jm}x_{ij} \\right)^2}{\\sum_{i=1}^p \\frac{1}{n}\\sum_{i=1}^n x_{im}^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deciding How Many Principal Components to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a $n\\times p$ data matrix X has $\\min(n âˆ’ 1, p)$ distinct principal\n",
    "components. However, we usually are not interested in all of them. In fact, we would like to use the smallest\n",
    "number of principal components required to get a good understanding of the\n",
    "data. We typically decide on the number of principal components required\n",
    "to visualize the data by examining a *scree plot*. We choose the smallest number of\n",
    "principal components that are required in order to explain a sizable amount\n",
    "of the variation in the data. This is done by eyeballing the scree plot, and\n",
    "looking for a point at which the proportion of variance explained by each\n",
    "subsequent principal component drops off. This is often referred to as an\n",
    "elbow in the scree plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering refers to a very broad set of techniques for finding subgroups, or\n",
    "clustering\n",
    "clusters, in a data set. When we cluster the observations of a data set, we\n",
    "seek to partition them into distinct groups so that the observations within\n",
    "each group are quite similar to each other, while observations in different\n",
    "groups are quite different from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Both clustering and PCA seek to simplify the data via a small number\n",
    "of summaries, but their mechanisms are different:\n",
    ">* PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance;\n",
    ">* Clustering looks to find homogeneous subgroups among the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering is a simple and elegant approach for partitioning a\n",
    "data set into K distinct, non-overlapping clusters. To perform K-means\n",
    "clustering, we must first specify the desired number of clusters K; then the\n",
    "K-means algorithm will assign each observation to exactly one of the K\n",
    "clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/10_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm for K-means clustering is as follows:\n",
    "1. Randomly assign a number, from 1 to K, to each of the observations.\n",
    "These serve as initial cluster assignments for the observations.\n",
    "2. Iterate until the cluster assignments stop changing:\n",
    "    - For each of the K clusters, compute the cluster centroid. The\n",
    "kth cluster centroid is the vector of the p feature means for the\n",
    "observations in the kth cluster.\n",
    "    - Assign each observation to the cluster whose centroid is closest\n",
    "(where closest is defined using Euclidean distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means algorithm finds a local rather than a global optimum and hence the results obtained will depend on the initial (random) cluster assignment of each observation in Step 1 of the Algorithm. For this reason,\n",
    "it is important to run the algorithm multiple times from different random initial configurations. Then one selects the best solution, i.e. that for which\n",
    "the objective is smallest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential disadvantage of K-means clustering is that it requires us to\n",
    "pre-specify the number of clusters K. Hierarchical clustering is an alternative approach which does not require that we commit to a particular\n",
    "choice of K. Hierarchical clustering has an added advantage over K-means\n",
    "clustering in that it results in an attractive tree-based representation of the\n",
    "observations, called a dendrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/10_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left-hand panel of the figure above, each leaf of the dendrogram represents one of the 45 observations. However, as we move\n",
    "up the tree, some leaves begin to fuse into branches. These correspond to\n",
    "observations that are similar to each other. As we move higher up the tree,\n",
    "branches themselves fuse, either with leaves or other branches. The earlier\n",
    "(lower in the tree) fusions occur, the more similar the groups of observations are to each other. On the other hand, observations that fuse later\n",
    "(near the top of the tree) can be quite different. In fact, this statement\n",
    "can be made precise: for any two observations, we can look for the point in\n",
    "the tree where branches containing those two observations are first fused.\n",
    "The height of this fusion, as measured on the vertical axis, indicates how different the two observations are. Thus, observations that fuse at the very\n",
    "bottom of the tree are quite similar to each other, whereas observations\n",
    "that fuse close to the top of the tree will tend to be quite different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Hierarchical Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hierarchical clustering dendrogram is obtained via an extremely simple\n",
    "algorithm. We begin by defining some sort of dissimilarity measure between\n",
    "each pair of observations. Most often, Euclidean distance is used. The algorithm proceeds iteratively. Starting out at the bottom of the dendrogram,\n",
    "each of the n observations is treated as its own cluster. The two clusters\n",
    "that are most similar to each other are then fused so that there now are\n",
    "nâˆ’ 1 clusters. Next the two clusters that are most similar to each other are\n",
    "fused again, so that there now are n âˆ’ 2 clusters. The algorithm proceeds\n",
    "in this fashion until all of the observations belong to one single cluster, and\n",
    "the dendrogram is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for this algorithm to work The concept of dissimilarity\n",
    "between a pair of observations needs to be extended to a pair of groups\n",
    "of observations. This extension is achieved by developing the notion of\n",
    "linkage, which defines the dissimilarity between two groups of observalinkage\n",
    "tions. The four most common types of linkageâ€”complete, average, single,\n",
    "and centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/10_04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm proceeds in the following manner:\n",
    "1. Begin with n observations and a measure (such as Euclidean distance) of all the $^nP_2 = n(n âˆ’ 1)/2$ pairwise dissimilarities. Treat each\n",
    "observation as its own cluster.\n",
    "1. For $i = n, n âˆ’ 1, \\ldots , 2$:\n",
    "    - Examine all pairwise inter-cluster dissimilarities among the i\n",
    "clusters and identify the pair of clusters that are least dissimilar\n",
    "(that is, most similar). Fuse these two clusters. The dissimilarity\n",
    "between these two clusters indicates the height in the dendrogram at which the fusion should be placed.\n",
    "    - Compute the new pairwise inter-cluster dissimilarities among\n",
    "the i âˆ’ 1 remaining clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/10_05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of Dissimilarity Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of dissimilarity measure is very important, as it has a strong\n",
    "effect on the resulting dendrogram. In general, careful attention should be\n",
    "paid to the type of data being clustered and the scientific question at hand.\n",
    "These considerations should determine what type of dissimilarity measure\n",
    "is used for hierarchical clustering."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
