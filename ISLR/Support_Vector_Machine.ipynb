{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximal Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a p-dimensional space, a hyperplane is a flat affine subspace of\n",
    "hyperplane\n",
    "dimension p − 1. For instance, in two dimensions, a hyperplane is a flat\n",
    "one-dimensional subspace—in other words, a line. In three dimensions, a\n",
    "hyperplane is a flat two-dimensional subspace—that is, a plane. In p-dimensions, a hyperplane is defined as:\n",
    "$$\n",
    "\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also think of the\n",
    "hyperplane as dividing p-dimensional space into two halves. One can easily\n",
    "determine on which side of the hyperplane a point lies by simply calculating\n",
    "the sign of the left hand side of the above equation. A two dimensional hyperplane is shown in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/09_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Using a Separating Hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we have a $n\\times p$ data matrix X that consists of n training\n",
    "observations in p-dimensional space. These observations falls into two categories: 1 and -1. Suppose that it is possible to construct a hyperplane that separates the\n",
    "training observations perfectly according to their class labels. That is, we have a hyperplane such that:\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} > 0 \\quad \\text{if} \\quad y_{i} = 1\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} < 0 \\quad \\text{if} \\quad y_{i} = -1\n",
    "$$\n",
    "This is equivalent to:\n",
    "$$\n",
    "y_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}) > 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a separating hyperplane exists, we can use it to construct a very natural\n",
    "classifier: a test observation is assigned a class depending on which side of\n",
    "the hyperplane it is located. We can even use the value $f(x) = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_p x_{p}$ to determining how \"sure\" the classifier is about the class of the test observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/09_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Maximal Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, if our data can be perfectly separated using a hyperplane, then\n",
    "there will in fact exist an infinite number of such hyperplanes. In order to construct a classifier based upon a separating\n",
    "hyperplane, we must have a reasonable way to decide which of the infinite\n",
    "possible separating hyperplanes to use.\n",
    "\n",
    "A natural choice is the maximal margin hyperplane, which is the separating hyperplane that\n",
    "is farthest from the training observations. That is, we can compute the\n",
    "(perpendicular) distance from each training observation to a given separating hyperplane; the smallest such distance is the minimal distance from the\n",
    "observations to the hyperplane, and is known as the margin. The maximal\n",
    "margin\n",
    "margin hyperplane is the separating hyperplane for which the margin is\n",
    "largest—that is, it is the hyperplane that has the farthest minimum distance to the training observations. We can then classify a test observation\n",
    "based on which side of the maximal margin hyperplane it lies. This is known\n",
    "as the maximal margin classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/09_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we see that three training observations are equidistant from the maximal margin hyperplane and lie along the dashed lines\n",
    "indicating the width of the margin. These three observations are known as *support vectors*, since they are vectors in p-dimensional space and they “support” the maximal margin hyperplane in the sense vector\n",
    "that if these points were moved slightly then the maximal margin hyperplane would move as well. Interestingly, the maximal margin hyperplane\n",
    "depends directly on the support vectors, but not on the other observations:\n",
    "a movement to any of the other observations would not affect the separating\n",
    "hyperplane, provided that the observation’s movement does not cause it to\n",
    "cross the boundary set by the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construction of the Maximal Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the hyperplane constraint to:\n",
    "$$\n",
    "y_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}) \\ge M \\quad \\forall i=1,2,\\dots,n\n",
    "$$\n",
    "the constraint requires that each\n",
    "observation be on the correct side of the hyperplane, with some cushion,\n",
    "provided that M is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many times the hyperplane that separates the training observations does not exist. Furthermore, maximal margin hyperplane is extremely sensitive to a change in a single observation\n",
    "suggests that it may have overfit the training data. In this case, we might be willing to consider a classifier based on a hyperplane that does not perfectly separate the two classes, in the interest of:\n",
    "* Greater robustness to individual observations, and\n",
    "* Better classification of most of the training observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector classifier, sometimes called a soft margin classifier,\n",
    "does exactly this. Rather than seeking the largest possible margin so that\n",
    "every observation is not only on the correct side of the hyperplane but\n",
    "also on the correct side of the margin, we instead allow some observations\n",
    "to be on the incorrect side of the margin, or even the incorrect side of\n",
    "the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details of the Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector classifier classifies a test observation depending on\n",
    "which side of a hyperplane it lies. The hyperplane is chosen to correctly separate most of the training observations into the two classes, but may\n",
    "misclassify a few observations. It is the solution to the optimization problem:\n",
    "$$\n",
    "\\text{maximize}_{\\beta_0, \\beta_1, \\cdots, \\beta_p, \\epsilon_0, \\epsilon_1, \\cdots, \\epsilon_p} M\n",
    "$$\n",
    "subjected to the constraints:\n",
    "$$\n",
    "\\sum_{j=1}^p\\beta_j^2 = 1\\\\\n",
    "y_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}) \\ge M(1-\\epsilon_i)\\\\\n",
    "\\epsilon_i \\ge 0, \\sum_{j=1}^n\\epsilon_j = C\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here C is a nonnegative tuning parameter. M is the width\n",
    "of the margin; we seek to make this quantity as large as possible. $\\epsilon_1, \\cdots, \\epsilon_n$ are slack variables that allow individual observations to be on\n",
    "slack\n",
    "the wrong side of the margin or the hyperplane.\n",
    "\n",
    "the slack variable $\\epsilon_i$ tells us where the ith observation is located,\n",
    "relative to the hyperplane and relative to the margin. If $\\epsilon_i=0$ then the ith observation is on the correct side of the margin. If $\\epsilon_i > 0$ then the ith observation is on the wrong side of the margin, and\n",
    "we say that the ith observation has violated the margin. If $\\epsilon_i > 1$ then it\n",
    "is on the wrong side of the hyperplane.\n",
    "\n",
    "C bounds\n",
    "the sum of the $\\epsilon_i$’s, and so it determines the number and severity of the violations to the margin (and to the hyperplane) that we will tolerate. We can\n",
    "think of C as a budget for the amount that the margin can be violated\n",
    "by the n observations. For C > 0 no more than C observations can be on the wrong side of the hyperplane. C controls the bias-variance trade-off of the support\n",
    "vector classifier. When the tuning parameter C is large, then the margin is\n",
    "wide, many observations violate the margin, and so there are many support\n",
    "vectors. In this case, many observations are involved in determining the\n",
    "hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization problem has a very interesting property:\n",
    "it turns out that only observations that either lie on the margin or that\n",
    "violate the margin will affect the hyperplane, and hence the classifier obtained. In other words, an observation that lies strictly on the correct side\n",
    "of the margin does not affect the support vector classifier! Changing the\n",
    "position of that observation would not change the classifier at all, provided\n",
    "that its position remains on the correct side of the margin. Observations\n",
    "that lie directly on the margin, or on the wrong side of the margin for\n",
    "their class, are known as support vectors. These observations do affect the\n",
    "support vector classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/09_04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows classifiers fit with different values of C. Largest C corresponds to the top-left image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector classifier is a natural approach for classification in the\n",
    "two-class setting, if the boundary between the two classes is linear. However, in practice we are sometimes faced with non-linear class boundaries. A natural solution to this problem is to enlarge the feature space quadratic or cubic terms. For instance, rather than fitting a\n",
    "support vector classifier using p features:\n",
    "$$\n",
    "X_1, X_2, \\cdots, X_p \n",
    "$$\n",
    "we could instead fit a support vector classifier using 2p features:\n",
    "$$\n",
    "X_1, X_1^2, X_2, X_2^2, \\cdots, X_n, X_p^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector machine (SVM) is an extension of the support vector\n",
    "classifier that results from enlarging the feature space in a specific way,\n",
    "using kernels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A kernel is a\n",
    "function that quantifies the similarity of two observations. Suppose we have two vectors $x_i, x_{i^{'}}$. The inner product of these two vectors can be computed as\n",
    "$$\n",
    "\\langle x_i, x_{i^{'}} \\rangle = \\sum_{j=1}^p x_{ij} x_{{i}^{'}j}\n",
    "$$ \n",
    "This also forms a kernel, called a linear kernel:\n",
    "$$\n",
    "K(x_i, x_{i^{'}}) = \\sum_{j=1}^p x_{ij} x_{{i}^{'}j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could generalize the linear kernel as:\n",
    "$$\n",
    "K(x_i, x_{i^{'}}) =(1+\\sum_{j=1}^p x_{ij} x_{{i}^{'}j})^d\n",
    "$$\n",
    "which is a polynoimal kernel of order d. Using such a kernel with d > 1, instead of the standard linear kernel in the support vector classifier algorithm leads to a much more\n",
    "flexible decision boundary. It essentially amounts to fitting a support vector\n",
    "classifier in a higher-dimensional space involving polynomials of degree d,\n",
    "rather than in the original feature space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radial Basis Function (RBF) Kernel, another popular kernel takes form:\n",
    "$$\n",
    "K(x_i, x_{i^{'}}) = e^{-\\gamma \\sum_{j=1}^{p}(x_i - x_{i^{'}j})^2}\n",
    "$$\n",
    "Where $\\gamma$, a positive constant, is a parameter that controls the width of the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/09_05.png)\n",
    "Polynomial Kernel and Radial Basis Function (RBF) Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVMs with More than Two Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the concept of separating hyperplanes upon which SVMs\n",
    "are based does not lend itself naturally to more than two classes. Though\n",
    "a number of proposals for extending SVMs to the K-class case have been\n",
    "made, the two most popular are the *one-versus-one* and *one-versus-all*\n",
    "approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Versus-One Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we would like to perform classification using SVMs, and there\n",
    "are K > 2 classes. A one-versus-one or all-pairs approach constructs $^kP_2$. SVMs, each of which compares a pair of classes. For example, one such\n",
    "SVM might compare the kth class, coded as +1, to the $k^{'}$ th class, coded\n",
    "as −1. We classify a test observation using each of the $^kP_2$ classifiers, and\n",
    "we tally the number of times that the test observation is assigned to each\n",
    "of the K classes. The final classification is performed by assigning the test\n",
    "observation to the class to which it was most frequently assigned in these\n",
    "$^kP_2$ pairwise classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Versus-All Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-versus-all approach is an alternative procedure for applying SVMs in the case of K > 2 classes. We fit K SVMs, each time comparing one of all\n",
    "the K classes to the remaining K − 1 classes. Let $\\beta_{0k}, \\beta_{1k}, \\cdots , \\beta_{pk}$ denote\n",
    "the parameters that result from fitting an SVM comparing the kth class\n",
    "(coded as +1) to the others (coded as −1). Let $x^∗$ denote a test observation.\n",
    "We assign the observation to the class for which $\\beta_{0k} +\\beta_{1k}{x^∗_1} +\\beta_{2k}{x^∗_2} +\\cdots+\\beta_{pk}{x^∗_p}$ is largest, as this amounts to a high level of confidence that the test\n",
    "observation belongs to the kth class rather than to any of the other classes"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
