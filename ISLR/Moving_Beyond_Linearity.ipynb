{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Contents\">Contents<a href=\"#Contents\"></a></h2>\n",
    "        <ol>\n",
    "        <li><a class=\"\" href=\"#Moving-Beyond-Linearity\">Moving Beyond Linearity</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Polynomial-Regression\">Polynomial Regression</a></li>\n",
    "<li><a class=\"\" href=\"#Step-Functions\">Step Functions</a></li>\n",
    "<li><a class=\"\" href=\"#Basis-Functions\">Basis Functions</a></li>\n",
    "<li><a class=\"\" href=\"#Regression-Splines\">Regression Splines</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Piecewise-Polynomials\">Piecewise Polynomials</a></li>\n",
    "<li><a class=\"\" href=\"#Constraints-and-Splines\">Constraints and Splines</a></li>\n",
    "<li><a class=\"\" href=\"#Choosing-the-Number-and-Locations-of-the-Knots\">Choosing the Number and Locations of the Knots</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Smoothing-Splines\">Smoothing Splines</a></li>\n",
    "<li><a class=\"\" href=\"#Local-Regression\">Local Regression</a></li>\n",
    "<li><a class=\"\" href=\"#Generalized-Additive-Models\">Generalized Additive Models</a></li>\n",
    "<ol><li><a class=\"\" href=\"#GAMs-for-Regression-Problems\">GAMs for Regression Problems</a></li>\n",
    "<li><a class=\"\" href=\"#GAMs-for-Classification-Problems\">GAMs for Classification Problems</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Beyond Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models\n",
    "are relatively simple to describe and implement, and have advantages over\n",
    "other approaches in terms of interpretation and inference. However, standard linear regression can have significant limitations in terms of predictive power. This is because the linearity assumption is almost always an\n",
    "approximation, and sometimes a poor one.\n",
    "\n",
    "Here, we relax the linearity assumption\n",
    "while still attempting to maintain as much interpretability as possible. We\n",
    "do this by examining very simple extensions of linear models like polynomial regression and step functions, as well as more sophisticated approaches\n",
    "such as splines, local regression, and generalized additive models.\n",
    "* **Polynomial regression** extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power.\n",
    "For example, a cubic regression uses three variables, X, X2, and X3,\n",
    "as predictors. This approach provides a simple way to provide a nonlinear fit to data.\n",
    "* **Step functions** cut the range of a variable into K distinct regions in\n",
    "order to produce a qualitative variable. This has the effect of fitting\n",
    "a piecewise constant function.\n",
    "* **Regression splines** are more flexible than polynomials and step\n",
    "functions, and in fact are an extension of the two. They involve dividing the range of X into K distinct regions. Within each region,\n",
    "a polynomial function is fit to the data. However, these polynomials\n",
    "are constrained so that they join smoothly at the region boundaries,\n",
    "or knots. Provided that the interval is divided into enough regions,\n",
    "this can produce an extremely flexible fit.\n",
    "* **Smoothing splines** are similar to regression splines, but arise in a\n",
    "slightly different situation. Smoothing splines result from minimizing\n",
    "a residual sum of squares criterion subject to a smoothness penalty.\n",
    "* **Local regression** is similar to splines, but differs in an important way.\n",
    "The regions are allowed to overlap, and indeed they do so in a very\n",
    "smooth way.\n",
    "* **Generalized additive models** allow us to extend the methods above to\n",
    "deal with multiple predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A polynomial model is\n",
    "$$\n",
    "y_i = \\sum_{j=0}^d a_j x_i^j + \\epsilon_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $\\epsilon_i$ is the error term and d is the degree of the polynomial. The coefficients $\\{a_j\\}$ can easily be estimated by minimizing the sum of squares of the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using polynomial functions of the features as predictors in a linear model\n",
    "imposes a global structure on the non-linear function of X. We can instead\n",
    "use step functions in order to avoid imposing such a global structure. Here\n",
    "we break the range of X into bins, and fit a different constant in each bin.\n",
    "This amounts to converting a continuous variable into an ordered categorical\n",
    "variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create cutpoints $c_1, c_2, \\ldots, c_K$ in the range of X and then construct K+1 new variables\"\n",
    "$$\n",
    "\\begin{align*}\n",
    "   C_0(X) &= I(X<C_1) \\\\\n",
    "   C_1(X) &= I(c_1\\le X<c_2) \\\\\n",
    "    \\vdots \\\\\n",
    "    C_K(X) &= I(c_K\\le X) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Where I is an indicator function that returns 1 if the condition is true and 0 otherwise. We than use $C_i$ as predictors in a linear model.\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 C_1(X_i) + \\beta_2 C_2(X_i) + \\ldots + \\beta_K C_K(X_i) + \\epsilon_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of basis functions is to use a set of predefined functions to fit a model. Let these functions be $b_1, b_2, \\ldots, b_K$. We then fit the model:\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 b_1(X_i) + \\beta_2 b_2(X_i) + \\ldots + \\beta_K b_K(X_i) + \\epsilon_i\n",
    "$$\n",
    "We can se least squares to estimate the unknown regression coefficients of the model defined above.\n",
    "\n",
    "Note that the polynomial model as well as the step functions are a special case of a basis function model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piecewise Polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of fitting a high-degree polynomial over the entire range of X, piecewise polynomial regression involves fitting separate low-degree polynomials regression\n",
    "over different regions of X. For example, a piecewise cubic polynomial works\n",
    "by fitting a cubic regression model of the form:\n",
    "$$\n",
    "y_i = \\beta_0+\\beta_1 x_i+\\beta_2 x_i^2+\\beta_3 x_i^3+\\epsilon_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients $\\{\\beta_j\\}$ differ in different regions of X called knots. Using more knots leads to a more flexible piecewise polynomial. In general, if we place K different knots throughout the range of X, then we\n",
    "will end up fitting K + 1 different cubic polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints and Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/07_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the image, we can see that just fitting a cubic polynomial is not enough. It created discontinuities. (Top-left image)\n",
    "\n",
    "We can fix the discontinuities by adding some constraints. The top-right image fits a cubic polynomial with constraint that it is continuous at the knots. The bottom-left image uses further constraints that the first and second derivatives are also continuous at the knots. The curve in the bottom-left is called a cubic spline. In general, a cubic spline with K knots uses\n",
    "cubic spline\n",
    "a total of 4 + K degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Number and Locations of the Knots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression\n",
    "spline is most flexible in regions that contain a lot of knots, because in\n",
    "those regions the polynomial coefficients can change rapidly. Hence, one\n",
    "option is to place more knots in places where we feel the function might\n",
    "vary most rapidly, and to place fewer knots where it seems more stable.\n",
    "While this option can work well, in practice it is common to place knots in\n",
    "a uniform fashion. One way to do this is to specify the desired degrees of\n",
    "freedom, and then have the software automatically place the corresponding\n",
    "number of knots at uniform quantiles of the data.\n",
    "\n",
    "How many knots should we use, or equivalently how many degrees of\n",
    "freedom should our spline contain? One option is to try out different numbers of knots and see which produces the best looking curve. A somewhat\n",
    "more objective approach is to use cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fitting a smooth curve to a set of data, what we really want to do is\n",
    "find some function, say $g(x)$, that fits the observed data well. However, if we don't put any constraint on the function, it will overfit the data. What we really want is a function g that makes RSS small,\n",
    "but that is also smooth. This can be done by finding g such that it minimizes the following cost function:\n",
    "$$\n",
    "\\sum_{i=1}^n(y_i-g(x_i))^2+\\lambda \\int g^{''}(t)^2 dt\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here $\\lambda$ is a non-negative tuning parameter.\n",
    "\n",
    "The function g(x) that minimizes the cost function can be shown to have some special properties: it is a piecewise cubic polynomial with knots at the unique\n",
    "values of $x_1, \\cdots , x_n$, and continuous first and second derivatives at each\n",
    "knot. Furthermore, it is linear in the region outside of the extreme knots.\n",
    "In other words, the function g(x) that minimizes the cost function is a natural cubic\n",
    "spline with knots at $x_1, \\cdots , x_n$! However, it is not the same natural cubic\n",
    "spline that one would get if one applied the basis function approach with knots at $x_1, \\cdots , x_n$â€”rather, it is a shrunken\n",
    "version of such a natural cubic spline, where the value of the tuning parameter $\\lambda$ in the cost function controls the level of shrinkage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local regression is a different approach for fitting flexible non-linear funclocal\n",
    "tions, which involves computing the fit at a target point $x_0$ using only the regression\n",
    "nearby training observations. The algorithm is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gather the fraction $s = k/n$ of training points whose $x_i$ are closest\n",
    "to $x_0$.\n",
    "2. Assign a weight $K_{i0} = K(x_i, x_0)$ to each point in this neighborhood,\n",
    "so that the point furthest from $x_0$ has weight zero, and the closest\n",
    "has the highest weight. All but these k nearest neighbors get weight\n",
    "zero.\n",
    "3. Fit a weighted least squares regression of the $y_i$ on the $x_i$ using the\n",
    "aforementioned weights, by finding $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ that minimize\n",
    "$$\n",
    "\\sum_{i=1}^nK_{i0}(y_i-\\hat{y_i})^2\\\\\n",
    "\\hat{y_i} = \\hat{\\beta_0}+\\hat{\\beta_1}x_i\n",
    "$$\n",
    "4. The fitted value as $x_0$ is given by $\\hat f{x_0} = \\hat{\\beta_0}+\\hat{\\beta_1}x_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform local regression, there are a number of choices to be\n",
    "made, such as how to define the weighting function K, and whether to fit\n",
    "a linear, constant, or quadratic regression in Step 3 above. While all of these choices make some\n",
    "difference, the most important choice is the span s, defined in Step 1 above.\n",
    "The span plays a role like that of the tuning parameter $\\lambda$ in smoothing\n",
    "splines: it controls the flexibility of the non-linear fit. The smaller the value\n",
    "of s, the more local and wiggly will be our fit; alternatively, a very large\n",
    "value of s will lead to a global fit to the data using all of the training\n",
    "observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Additive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upto now, we have discussed a number of approaches for flexibly predicting a response Y on the basis of a single predictor X. These approaches can\n",
    "be seen as extensions of simple linear regression. Here we explore the problem of flexibly predicting Y on the basis of several predictors, $X_1, \\ldots , X_p$.\n",
    "This amounts to an extension of multiple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAMs for Regression Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the multiple linear regression model:\n",
    "$$\n",
    "\\hat{y_i} = \\hat{\\beta_0}+\\hat{\\beta_1}x_{i1}+\\hat{\\beta_2}x_{i2}+\\cdots+\\hat{\\beta_p}x_{ip} + \\epsilon_i\n",
    "$$\n",
    "in order to allow for non-linear relationships between each feature and the\n",
    "response is to replace each linear component $\\beta_jx_{ij}$ with a (smooth) nonlinear function $f_j(x_{ij})$. We would then write the model as:\n",
    "$$\n",
    "\\hat{y_i} = \\hat{\\beta_0} + f_1(x_{i1})+f_2(x_{i2})+\\cdots+f_p(x_{ip}) + \\epsilon_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a GAM. It is called an additive model because we\n",
    "calculate a separate $f_j$ for each $X_j$, and then add together all of their\n",
    "contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAMs for Classification Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GAM model can be used for classification problems too. We extend the existing logistic regression model:\n",
    "$$\n",
    "\\log \\left(\\frac{p(X)}{1-p(X)}\\right) = \\hat{\\beta_0}+\\hat{\\beta_1}X_{1}+\\hat{\\beta_2}X_{2}+\\cdots+\\hat{\\beta_p}X_{p}\n",
    "$$\n",
    "to a GAM model:\n",
    "$$\n",
    "\\log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\hat{\\beta_0}+f_1(X_{1})+f_2(X_{2})+\\cdots+f_p(X_{p})\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
