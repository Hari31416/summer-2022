{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Contents\">Contents<a href=\"#Contents\"></a></h2>\n",
    "        <ol>\n",
    "        <li><a class=\"\" href=\"#Tree-Based-Methods\">Tree-Based Methods</a></li>\n",
    "<ol><li><a class=\"\" href=\"#The-Basics-of-Decision-Trees\">The Basics of Decision Trees</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Regression-Trees\">Regression Trees</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Tree-Pruning\">Tree Pruning</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Classification-Trees\">Classification Trees</a></li>\n",
    "<li><a class=\"\" href=\"#Trees-Versus-Linear-Models\">Trees Versus Linear Models</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Bagging\">Bagging</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Out-of-Bag-Error-Estimation\">Out-of-Bag Error Estimation</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Random-Forests\">Random Forests</a></li>\n",
    "<li><a class=\"\" href=\"#Boosting\">Boosting</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree-based methods involve stratifying or segmenting the predictor space\n",
    "into a number of simple regions. In order to make a prediction for a given\n",
    "observation, we typically use the mean or the mode of the training observations in the region to which it belongs. Since the set of splitting rules used\n",
    "to segment the predictor space can be summarized in a tree, these types of\n",
    "approaches are known as decision tree methods.\n",
    "decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basics of Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly speaking,\n",
    "there are two steps to form a decision tree:\n",
    "1. We divide the predictor space—that is, the set of possible values for\n",
    "$X_1, X_2, \\ldots , X_p$—into J distinct and non-overlapping regions,\n",
    "$R_1, R_2, \\ldots , R_J$.\n",
    "2. For every observation that falls into the region $R_j$, we make the same\n",
    "prediction, which is simply the mean of the response values for the\n",
    "training observations in $R_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we construct the regions $R_1, R_2, \\ldots , R_J$? The goal is to find these regions such that they minimize the RSS error:\n",
    "$$\n",
    "\\text{RSS}(R_j) = \\sum_{i=1}^N \\left( y_i - \\hat{y_{R_j}} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it is computationally infeasible to consider every\n",
    "possible partition of the feature space into J boxes. For this reason, we take\n",
    "a top-down, greedy approach that is known as recursive binary splitting. The\n",
    "recursive\n",
    "binary\n",
    "splitting\n",
    "approach is top-down because it begins at the top of the tree (at which point\n",
    "all observations belong to a single region) and then successively splits the\n",
    "predictor space; each split is indicated via two new branches further down\n",
    "on the tree. It is greedy because at each step of the tree-building process,\n",
    "the best split is made at that particular step, rather than looking ahead\n",
    "and picking a split that will lead to a better tree in some future step.\n",
    "\n",
    "In order to perform recursive binary splitting, we first select the predictor $X_j$ and the cutpoint s such that splitting the predictor space into\n",
    "the regions $\\{X|X_j < s \\}$ and $\\{X|X_j\\le s\\}$ leads to the greatest possible\n",
    "reduction in RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, we consider all\n",
    "predictors $X_1, X_2, \\ldots , X_p$, and all possible values of the cutpoint s for each of\n",
    "the predictors, and then choose the predictor and cutpoint such that the\n",
    "resulting tree has the lowest RSS. In greater detail, for any j and s, we\n",
    "define the pair of half-planes:\n",
    "$$\n",
    "R_1(j,s) = \\{X|X_j < s\\}\\quad \\text{and}\\quad R_2(j,s)= \\{X|X_j\\le s\\}\n",
    "$$\n",
    "and we seek j, s such that it minimizes:\n",
    "$$\n",
    "\\sum_{i:x_i\\epsilon R_1(j,s)}\\left( y_i - \\hat{y}_{R_1} \\right)^2 +\\sum_{i:x_i\\epsilon R_2(j,s)}\\left( y_i - \\hat{y}_{R_2} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we find the j and s, we repeat the process, looking for the best predictor and best\n",
    "cutpoint in order to split the data further so as to minimize the RSS within\n",
    "each of the resulting regions. However, this time, instead of splitting the\n",
    "entire predictor space, we split one of the two previously identified regions.\n",
    "We now have three regions. Again, we look to split one of these three regions\n",
    "further, so as to minimize the RSS. The process continues until a stopping\n",
    "criterion is reached; for instance, we may continue until no region contains\n",
    "more than five observations. Once the regions $R_1, R_2, \\ldots , R_J$ have been created, we predict the response\n",
    "for a given test observation using the mean of the training observations in\n",
    "the region to which that test observation belongs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process described above may produce good predictions on the training\n",
    "set, but is likely to overfit the data, leading to poor test set performance. One possible\n",
    "alternative to the process described above is to build the tree only so long\n",
    "as the decrease in the RSS due to each split exceeds some (high) threshold.\n",
    "This strategy will result in smaller trees, but is too short-sighted since a\n",
    "seemingly worthless split early on in the tree might be followed by a very\n",
    "good split—that is, a split that leads to a large reduction in RSS later on. Therefore, a better strategy is to grow a very large tree $T_0$, and then\n",
    "prune it back in order to obtain a subtree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cost complexity pruning*—also known as weakest link pruning—gives us\n",
    "a way to do just this. Rather than considering every possible subtree, we\n",
    "consider a sequence of trees indexed by a nonnegative tuning parameter $\\alpha$. For each value of $\\alpha$, we consider the subtree $T\\subset T_0$ such that:\n",
    "$$\n",
    "\\sum_{m=1}^{|T|}\\sum_{x_i\\epsilon R_m}\\left( y_i - \\hat{y}_{R_m} \\right)^2+\\alpha|T|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is as small as possible. We can select a value of\n",
    "$\\alpha$ using a validation set or using cross-validation. We then return to the\n",
    "full data set and obtain the subtree corresponding to $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The Algorithm\n",
    ">1. Use recursive binary splitting to grow a large tree on the training\n",
    "data, stopping only when each terminal node has fewer than some\n",
    "minimum number of observations.\n",
    ">2. Apply cost complexity pruning to the large tree in order to obtain a\n",
    "sequence of best subtrees, as a function of $\\alpha$.\n",
    ">3. Use K-fold cross-validation to choose $\\alpha$. That is, divide the training\n",
    "observations into K folds. For each $k = 1, \\ldots , K$:\n",
    "(a) Repeat Steps 1 and 2 on all but the kth fold of the training data.\n",
    "(b) Evaluate the mean squared prediction error on the data in the\n",
    "left-out kth fold, as a function of $\\alpha$.\n",
    "Average the results for each value of $\\alpha$, and pick $\\alpha$ to minimize the\n",
    "average error.\n",
    ">4. Return the subtree from Step 2 that corresponds to the chosen value\n",
    "of $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classification tree is very similar to a regression tree, except that it is\n",
    "classification\n",
    "used to predict a qualitative response rather than a quantitative one. Re- tree\n",
    "call that for a regression tree, the predicted response for an observation is\n",
    "given by the mean response of the training observations that belong to the\n",
    "same terminal node. In contrast, for a classification tree, we predict that\n",
    "each observation belongs to the most commonly occurring class of training\n",
    "observations in the region to which it belongs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To grow a classification tree, we use the same procedure as in the regression using *classification error rate* as the cost function.  The classification error rate is\n",
    "simply the fraction of the training observations in that region that do not\n",
    "belong to the most common class:\n",
    "$$\n",
    "E = 1-\\max_k(\\hat{p}_{mk})\n",
    "$$\n",
    "Here $\\hat{p}_{mk}$ represents the proportion of training observations in the mth\n",
    "region that are from the kth class. \n",
    "\n",
    "However, it turns out that classification\n",
    "error is not sufficiently sensitive for tree-growing, and in practice two other\n",
    "measures are preferable.\n",
    "The Gini index is defined by\n",
    "$$\n",
    "G = \\sum_{k=1}^K\\hat{p}_{mk}(1-\\hat{p}_{mk})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a measure of total variance across the K classes. It is not hard to see\n",
    "that the Gini index takes on a small value if all of the $\\hat{p}_{mk}$’s are close to\n",
    "zero or one. For this reason the Gini index is referred to as a measure of\n",
    "node purity—a small value indicates that a node contains predominantly\n",
    "observations from a single class.\n",
    "\n",
    "An alternative to the Gini index is cross-entropy, given by\n",
    "$$\n",
    "D = -\\sum_{k=1}^K\\hat{p}_{mk}\\log\\hat{p}_{mk}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can show that\n",
    "the cross-entropy will take on a value near zero if the $\\hat{p}_{mk}$’s are all near\n",
    "zero or near one. Therefore, like the Gini index, the cross-entropy will take\n",
    "on a small value if the mth node is pure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a classification tree, either the Gini index or the crossentropy are typically used to evaluate the quality of a particular split,\n",
    "since these two approaches are more sensitive to node purity than is the\n",
    "classification error rate. Any of these three approaches might be used when\n",
    "pruning the tree, but the classification error rate is preferable if prediction\n",
    "accuracy of the final pruned tree is the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Decision trees can be constructed\n",
    "even in the presence of qualitative predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees Versus Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression assumes a model of the form:\n",
    "$$\n",
    "f(X) = \\beta_0 + \\sum_{i=1}^n\\beta_iX_i\n",
    "$$\n",
    "whereas regression trees assume a model of the form:\n",
    "$$\n",
    "f(X) = \\sum_{m=1}^M c_m.1 (X\\epsilon R_m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model is better depends on the problem at hand. If the\n",
    "relationship between the features and the response is well approximated\n",
    "by a linear model then an approach such as linear regression\n",
    "will likely work well, and will outperform a method such as a regression\n",
    "tree that does not exploit this linear structure. If instead there is a highly\n",
    "non-linear and complex relationship between the features and the response\n",
    "as indicated by the tree model then decision trees may outperform classical\n",
    "approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees generally do not have the same level of predictive\n",
    "accuracy as some of the other regression and classification approaches however, by aggregating many decision trees, using methods like bagging,\n",
    "random forests, and boosting, the predictive performance of trees can be\n",
    "substantially improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree suffer from high variance.\n",
    "This means that if we split the training data into two parts at random,\n",
    "and fit a decision tree to both halves, the results that we get could be\n",
    "quite different. We know that averaging a set of observations reduces variance. Hence a natural way to reduce the variance and hence increase the prediction accuracy of a statistical learning method is to take many training sets\n",
    "from the population, build a separate prediction model using each training\n",
    "set, and average the resulting predictions. In other words, we could calculate $\\hat{f}^1(x), \\hat{f}^2(x), \\ldots, \\hat{f}^B(x)$ using B separate training sets, and average\n",
    "them in order to obtain a single low-variance statistical learning model,\n",
    "given by \n",
    "$$\n",
    "\\hat{f}^{\\text{avg}}(x) = \\frac{1}{B}\\sum_{i=1}^B\\hat{f}^i(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this is not practical because we generally do not have access\n",
    "to multiple training sets. Instead, we can bootstrap, by taking repeated samples from the single training set, and fit a decision tree to each sample. This is called bagging.\n",
    "\n",
    "To apply bagging to regression\n",
    "trees, we simply construct B regression trees using B bootstrapped training\n",
    "sets, and average the resulting predictions. These trees are grown deep,\n",
    "and are not pruned. Hence each individual tree has high variance, but\n",
    "low bias. Averaging these B trees reduces the variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-Bag Error Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can show\n",
    "that on average, each bagged tree makes use of around two-thirds of the\n",
    "observations. The remaining one-third of the observations not used to fit a\n",
    "given bagged tree are referred to as the out-of-bag (OOB) observations. We\n",
    "out-of-bag\n",
    "can predict the response for the ith observation using each of the trees in which that observation was OOB. This will yield around B/3 predictions\n",
    "for the ith observation. We can take average (or mode) which will give a valid test error for the model.  It can\n",
    "be shown that with B sufficiently large, OOB error is virtually equivalent\n",
    "to leave-one-out cross-validation error. The OOB approach for estimating\n",
    "the test error is particularly convenient when performing bagging on large\n",
    "data sets for which cross-validation would be computationally onerous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests provide an improvement over bagged trees by way of a\n",
    "random\n",
    "small tweak that decorrelates the trees. As in bagging, we build a number forest\n",
    "of decision trees on bootstrapped training samples. But when building these\n",
    "decision trees, each time a split in a tree is considered, a random sample of\n",
    "m predictors is chosen as split candidates from the full set of p predictors.\n",
    "The split is allowed to use only one of those m predictors. A fresh sample of\n",
    "m predictors is taken at each split, and typically we choose $m \\approx \\sqrt{p}$—that\n",
    "is, the number of predictors considered at each split is approximately equal\n",
    "to the square root of the total number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaving all these features may sound weird, but it has a clever rationale. Suppose\n",
    "that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged\n",
    "trees, most or all of the trees will use this strong predictor in the top split.\n",
    "Consequently, all of the bagged trees will look quite similar to each other.\n",
    "Hence the predictions from the bagged trees will be highly correlated.\n",
    "\n",
    "Random forests overcome this problem by forcing each split to consider\n",
    "only a subset of the predictors. Therefore, on average $(p − m)/p$ of the\n",
    "splits will not even consider the strong predictor, and so other predictors\n",
    "will have more of a chance. We can think of this process as decorrelating\n",
    "the trees, thereby making the average of the resulting trees less variable\n",
    "and hence more reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting works in a similar way as bagging, except that the trees are\n",
    "grown sequentially: each tree is grown using information from previously\n",
    "grown trees. Boosting does not involve bootstrap sampling; instead each\n",
    "tree is fit on a modified version of the original data set. The algorithm is:\n",
    "1. Set $\\hat{f}(x) = 0$ and $r_i = y_i$ for all i in the training set.\n",
    "2. For $b = 1, \\dots, B$,\n",
    "   - Fit a tree $\\hat{f}^b$ with d splits to the training data (X, r)\n",
    "   - Update $\\hat{f}$ by adding in a shrunken version of the new tree: $$\\hat{f}(x)\\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)$$\n",
    " - Update the residuals $$r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x_i)$$\n",
    "3. Output the final model: $$f(x) = \\sum_{b=1}^B\\lambda \\hat{f}^b(x)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially\n",
    "overfitting, the boosting approach instead learns slowly. Given the current\n",
    "model, we fit a decision tree to the residuals from the model. That is, we\n",
    "fit a tree using the current residuals, rather than the outcome Y , as the response. We then add this new decision tree into the fitted function in order\n",
    "to update the residuals. Each of these trees can be rather small, with just\n",
    "a few terminal nodes, determined by the parameter d in the algorithm. By\n",
    "fitting small trees to the residuals, we slowly improve $\\hat{f}$ in areas where it\n",
    "does not perform well. The shrinkage parameter $\\lambda$ slows the process down\n",
    "even further, allowing more and different shaped trees to attack the residuals. In general, statistical learning approaches that learn slowly tend to\n",
    "perform well. Note that in boosting, unlike in bagging, the construction of\n",
    "each tree depends strongly on the trees that have already been grown."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
