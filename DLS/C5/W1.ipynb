{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent\tNeural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uses of Sequence Models\n",
    "1. Speech Recognition\n",
    "2. Text Generation\n",
    "3. Music Generation\n",
    "4. Sentiment Analysis\n",
    "5. Language Translation\n",
    "6. DNA Sequence Analysis\n",
    "7. Video Analysis\n",
    "8. Named Entity Recognition\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the input be \"Harry Potter and Hermione Granger invented a new spell.\" We want a sequence model to automatically tell you where are the peoples names in this sentence. So, this is a problem called Named-entity recognition. We will use $x^{<t>}$ to denote the $t^{th}$ word in the input sequence. Similary $y^{<t>}$ denotes the $t^{th}$ output. Furthermore deote $T_x$ and $T_y$ as the length of the input and output sequence respectively. In our example both $T_x$ and $T_y$ are equal to $9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent a word in the sentence the first thing you do is come up with a Vocabulary. Every word in the vocabulary is represented by a unique integer. We then use one-hot encoding to convert them into an array which can be fed into neural network. In our example, taking a 10000 word vocabulary, the different words are reprensented by:\n",
    "* A = 1\n",
    "* And = 367\n",
    "* Harry = 4075\n",
    "* Hermione = 4200 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural\tNetwork\tModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, as a starting point, use a simple nueral network abd feed it with the array of one-hot encoded words. However, this approach does not work well. The main problems are:\n",
    "* Inputs, outputs can be different lengths in different examples.\n",
    "- Doesnâ€™t share features learned across different positions of text.\n",
    "* The weights matrix becomes huge.\n",
    "\n",
    "Due to these issues, we need to use a recurrent neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Does RNN Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN \"reads\" the input sequence one word at a time. At any time t, it looks at the current word and the activation corresponding to the previous word. Using these two information, it makes tha prediction. While \"reading\" the first input, we use a zero intialized zeroth activation vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/01_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward propagation of the RNN is governed by:\n",
    "$$\n",
    "a^{<t>} = g_1(W_{aa}a_{t-1} +W_{ax}x^{<t>} + b_a) \\\\\n",
    "\\hat{y}^{<t>} = g_2(W_{ya}a^{<t>} + b_y)\n",
    "$$\n",
    "This can be simplified to:\n",
    "$$\n",
    "a^{<t>} = g_1(W_{a}[a^{<t-1>}, x^{<t>}] + b_a) \\\\\n",
    "\\hat{y}^{<t>} = g_2(W_{y}a^{<t>} + b_y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here \n",
    "$$\n",
    "W_a = \\begin{bmatrix}\n",
    "W_{aa} & W_{ax} \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "$$\n",
    "$$\n",
    "[a^{<t-1>}, x^{<t>}] = \\begin{bmatrix}\n",
    "a^{<t-1>} &  \\\\\n",
    "x^{<t>} & \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\tThrough\tTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/01_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/01_03.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
