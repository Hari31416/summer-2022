{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Superscript $(i)$ will denote $i^{th}$ training example while superscript $[l]$ will denote the $l^{th}$ layer.\n",
    "\n",
    "**Sizes**\n",
    "* m: number of training examples\n",
    "* $n_x$: input size\n",
    "* $n_y$: output size\n",
    "* $n_h^{[l]}$: number of hidden units of $l^th$ layer\n",
    "* L : Number of layers\n",
    "\n",
    "**Objects**\n",
    "* $X$: input data, of shape $(n_x, m)$ (At other places, you'll se the order reversed)\n",
    "* $x^{(i)}$: $i^{th}$ training example, a $1 \\times n_x$ column vector\n",
    "* $Y$: labels matrix, of shape $(n_y, m)$\n",
    "* $y^{(i)}$: $i^{th}$ training label, a $n_y \\times 1$ column vector\n",
    "* $W^{[l]}$: weight matrix of shape $(n_{l}, n_{l-1})$\n",
    "* $b^{[l]}$: bias vector of shape $(n_{l}, 1)$\n",
    "* $\\hat{Y}$: prediction vector, a $n_y \\times 1$ column vector. Also denoted by $a^{[l]}$\n",
    "\n",
    "**forward propagation equation examples**\n",
    "$$ a= g^{[l]}\\left( W^{[l]}a^{[l-1]}+ b^{[l]}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, given X, we want\n",
    "$$ \\hat{y} = P(y=1|X) $$\n",
    "Parameters are W and b. The output is\n",
    "$$ \\hat{y} = \\sigma(W^TX+b) $$\n",
    "Where\n",
    "$$ \\sigma(z) = \\frac{1}{1+e^{-z}} $$\n",
    "Is the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function, we use for logistic regression, is\n",
    "$$ J(w, b) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)} \\log \\left( \\hat{y}^{(i)} \\right) + (1-y^{(i)})\\log\\left(1-\\hat{y}^{(i)} \\right) \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a cost function and we need to minimize it. We will use gradient descent to find the optimal parameters. For this, we have:\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J(w,b)}{\\partial w}\\\\\n",
    "b := b - \\alpha \\frac{\\partial J(w,b)}{\\partial b}\n",
    "$$\n",
    "$\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have function of functions, we can use chain rule to calculate the gradients. For example, let us consider the following functions:\n",
    "$$\n",
    "u = bc\\\\\n",
    "v = a+u\\\\\n",
    "J = 3v\n",
    "$$\n",
    "We can calculate the gradients of J with respect to u by\n",
    "$$\n",
    "\\frac{d J}{d u} = \\frac{d J}{d v}\\frac{d v}{d u}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
